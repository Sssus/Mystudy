{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tensorflow_overview.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"AHbFyJDsRrdM","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xa6P62RdRt9K","colab_type":"code","colab":{}},"source":["x_data = [1,2,3,4,5]\n","y_data = [10,20,30,40,50]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsDCKGJGRvCv","colab_type":"code","colab":{}},"source":["W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n","b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9sQBe2gR1GH","colab_type":"code","outputId":"25cb0022-faaa-4b1e-ff2f-7157346f9ec4","executionInfo":{"status":"ok","timestamp":1569915889966,"user_tz":-540,"elapsed":662,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(W)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rfKTkZUOR2Y_","colab_type":"code","outputId":"ca359e23-cd49-4f39-8b38-9059e64727ee","executionInfo":{"status":"ok","timestamp":1569915897239,"user_tz":-540,"elapsed":627,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(b)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H329L6YkR4K_","colab_type":"code","outputId":"9242f266-3ee6-4ec6-9da7-e50928bdb993","executionInfo":{"status":"ok","timestamp":1569915921368,"user_tz":-540,"elapsed":636,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["X = tf.placeholder(tf.float32, name='X')\n","Y = tf.placeholder(tf.float32, name='Y')\n","print(X)\n","print(Y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensor(\"X:0\", dtype=float32)\n","Tensor(\"Y:0\", dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jtaxMuBsR-D3","colab_type":"code","colab":{}},"source":["score = W*X + b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4go_0F9R_cv","colab_type":"code","outputId":"883f5440-9644-40ce-ae56-8620cbefb076","executionInfo":{"status":"ok","timestamp":1569915956466,"user_tz":-540,"elapsed":656,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["loss = tf.reduce_mean(tf.square(score - Y))\n","loss"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'Mean:0' shape=() dtype=float32>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"g6Xued9DSGn_","colab_type":"code","outputId":"81b5f0e2-366d-4771-ee74-db69b0c57e6e","executionInfo":{"status":"ok","timestamp":1569916034890,"user_tz":-540,"elapsed":627,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n","# GradientDescentOptimizer에 최적화 방법을 loss가 최소가 되게하는 minimize() 메서드 사용하기\n","train_op = optimizer.minimize(loss)\n","train_op"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Operation 'GradientDescent' type=NoOp>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Sd0ijfJiSZxn","colab_type":"code","outputId":"bcebf47b-e90f-4ed4-e3ad-1c8ca104854f","executionInfo":{"status":"ok","timestamp":1569916248424,"user_tz":-540,"elapsed":1344,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["with tf.Session() as sess:\n"," # 값을 초기화 하는 함수를 먼저 실행한다.\n"," sess.run(tf.global_variables_initializer())\n","\n"," # training data로 101번 학습 시키기\n"," # 학습시 경사하강법을 최적화 함수로 사용하고, loss 값을 최소가 되게 만든다.\n"," # 데이터는 플레이스홀더 X에는 x_data를 넣어주고\n"," # 플레이스 홀더 Y에는 y_data를 넣어준다.\n"," # 값을 2개 반환 받게 되는데 loss값만 loss_val 변수로 저장하자.\n"," for step in range(100):\n","  _, loss_val = sess.run([train_op,loss], feed_dict={X:x_data, Y:y_data})\n"," # 횟수, cost, W, B 찍어보기\n","  print(step, 'Loss = %-10.5f' % loss_val, 'W = %10.6f' % sess.run(W), 'B = %10.6f' % sess.run(b))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 Loss = 1315.45288 W =   1.538741 B =   0.406076\n","1 Loss = 767.07141  W =   3.375853 B =   0.905630\n","2 Loss = 447.49854  W =   4.778828 B =   1.284966\n","3 Loss = 261.26401  W =   5.850388 B =   1.572538\n","4 Loss = 152.73245  W =   6.668950 B =   1.790064\n","5 Loss = 89.48242   W =   7.294377 B =   1.954125\n","6 Loss = 52.62020   W =   7.772367 B =   2.077380\n","7 Loss = 31.13550   W =   8.137803 B =   2.169491\n","8 Loss = 18.61213   W =   8.417317 B =   2.237833\n","9 Loss = 11.31096   W =   8.631238 B =   2.288037\n","10 Loss = 7.05305    W =   8.795083 B =   2.324402\n","11 Loss = 4.56865    W =   8.920701 B =   2.350209\n","12 Loss = 3.11776    W =   9.017135 B =   2.367963\n","13 Loss = 2.26918    W =   9.091288 B =   2.379575\n","14 Loss = 1.77163    W =   9.148430 B =   2.386507\n","15 Loss = 1.47864    W =   9.192585 B =   2.389871\n","16 Loss = 1.30489    W =   9.226824 B =   2.390518\n","17 Loss = 1.20064    W =   9.253491 B =   2.389098\n","18 Loss = 1.13692    W =   9.274378 B =   2.386107\n","19 Loss = 1.09684    W =   9.290849 B =   2.381922\n","20 Loss = 1.07054    W =   9.303946 B =   2.376833\n","21 Loss = 1.05231    W =   9.314468 B =   2.371059\n","22 Loss = 1.03880    W =   9.323022 B =   2.364770\n","23 Loss = 1.02805    W =   9.330070 B =   2.358094\n","24 Loss = 1.01893    W =   9.335969 B =   2.351127\n","25 Loss = 1.01078    W =   9.340988 B =   2.343947\n","26 Loss = 1.00322    W =   9.345334 B =   2.336608\n","27 Loss = 0.99601    W =   9.349164 B =   2.329156\n","28 Loss = 0.98904    W =   9.352598 B =   2.321623\n","29 Loss = 0.98221    W =   9.355729 B =   2.314035\n","30 Loss = 0.97550    W =   9.358626 B =   2.306410\n","31 Loss = 0.96886    W =   9.361344 B =   2.298764\n","32 Loss = 0.96230    W =   9.363923 B =   2.291109\n","33 Loss = 0.95578    W =   9.366393 B =   2.283451\n","34 Loss = 0.94932    W =   9.368779 B =   2.275799\n","35 Loss = 0.94291    W =   9.371099 B =   2.268156\n","36 Loss = 0.93654    W =   9.373368 B =   2.260527\n","37 Loss = 0.93022    W =   9.375596 B =   2.252914\n","38 Loss = 0.92393    W =   9.377790 B =   2.245320\n","39 Loss = 0.91770    W =   9.379957 B =   2.237746\n","40 Loss = 0.91150    W =   9.382102 B =   2.230194\n","41 Loss = 0.90535    W =   9.384228 B =   2.222664\n","42 Loss = 0.89924    W =   9.386338 B =   2.215157\n","43 Loss = 0.89317    W =   9.388434 B =   2.207673\n","44 Loss = 0.88714    W =   9.390518 B =   2.200214\n","45 Loss = 0.88115    W =   9.392591 B =   2.192778\n","46 Loss = 0.87520    W =   9.394654 B =   2.185367\n","47 Loss = 0.86929    W =   9.396708 B =   2.177981\n","48 Loss = 0.86342    W =   9.398754 B =   2.170619\n","49 Loss = 0.85759    W =   9.400791 B =   2.163281\n","50 Loss = 0.85181    W =   9.402821 B =   2.155968\n","51 Loss = 0.84606    W =   9.404842 B =   2.148679\n","52 Loss = 0.84035    W =   9.406857 B =   2.141415\n","53 Loss = 0.83467    W =   9.408863 B =   2.134176\n","54 Loss = 0.82904    W =   9.410863 B =   2.126960\n","55 Loss = 0.82344    W =   9.412855 B =   2.119769\n","56 Loss = 0.81788    W =   9.414841 B =   2.112603\n","57 Loss = 0.81236    W =   9.416820 B =   2.105460\n","58 Loss = 0.80688    W =   9.418792 B =   2.098342\n","59 Loss = 0.80143    W =   9.420757 B =   2.091247\n","60 Loss = 0.79602    W =   9.422716 B =   2.084177\n","61 Loss = 0.79065    W =   9.424668 B =   2.077131\n","62 Loss = 0.78531    W =   9.426614 B =   2.070108\n","63 Loss = 0.78001    W =   9.428553 B =   2.063109\n","64 Loss = 0.77474    W =   9.430485 B =   2.056134\n","65 Loss = 0.76951    W =   9.432410 B =   2.049182\n","66 Loss = 0.76432    W =   9.434329 B =   2.042253\n","67 Loss = 0.75916    W =   9.436241 B =   2.035349\n","68 Loss = 0.75403    W =   9.438148 B =   2.028467\n","69 Loss = 0.74894    W =   9.440047 B =   2.021609\n","70 Loss = 0.74389    W =   9.441940 B =   2.014774\n","71 Loss = 0.73887    W =   9.443827 B =   2.007962\n","72 Loss = 0.73388    W =   9.445707 B =   2.001173\n","73 Loss = 0.72892    W =   9.447581 B =   1.994408\n","74 Loss = 0.72400    W =   9.449449 B =   1.987664\n","75 Loss = 0.71912    W =   9.451310 B =   1.980944\n","76 Loss = 0.71426    W =   9.453165 B =   1.974247\n","77 Loss = 0.70944    W =   9.455014 B =   1.967572\n","78 Loss = 0.70465    W =   9.456857 B =   1.960920\n","79 Loss = 0.69990    W =   9.458694 B =   1.954290\n","80 Loss = 0.69517    W =   9.460524 B =   1.947682\n","81 Loss = 0.69048    W =   9.462347 B =   1.941097\n","82 Loss = 0.68582    W =   9.464165 B =   1.934534\n","83 Loss = 0.68119    W =   9.465977 B =   1.927994\n","84 Loss = 0.67659    W =   9.467782 B =   1.921475\n","85 Loss = 0.67202    W =   9.469582 B =   1.914979\n","86 Loss = 0.66748    W =   9.471375 B =   1.908504\n","87 Loss = 0.66298    W =   9.473162 B =   1.902052\n","88 Loss = 0.65850    W =   9.474943 B =   1.895621\n","89 Loss = 0.65406    W =   9.476718 B =   1.889212\n","90 Loss = 0.64964    W =   9.478487 B =   1.882825\n","91 Loss = 0.64526    W =   9.480250 B =   1.876459\n","92 Loss = 0.64090    W =   9.482008 B =   1.870115\n","93 Loss = 0.63658    W =   9.483759 B =   1.863792\n","94 Loss = 0.63228    W =   9.485504 B =   1.857491\n","95 Loss = 0.62801    W =   9.487244 B =   1.851211\n","96 Loss = 0.62377    W =   9.488977 B =   1.844952\n","97 Loss = 0.61956    W =   9.490705 B =   1.838714\n","98 Loss = 0.61538    W =   9.492428 B =   1.832498\n","99 Loss = 0.61122    W =   9.494143 B =   1.826302\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B-uzrDIHTGCX","colab_type":"code","outputId":"1e1f0129-88c0-464b-f8da-bad3bacbc5ad","executionInfo":{"status":"ok","timestamp":1569916581929,"user_tz":-540,"elapsed":1030,"user":{"displayName":"eunsu Kim","photoUrl":"","userId":"02390913583849598770"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["with tf.Session() as sess:\n"," # 값을 초기화 하는 함수를 먼저 실행한다.\n","  sess.run(tf.global_variables_initializer())\n","\n"," # 0~1000 까지 1001번 학습시킨다.\n"," # 학습시 경사하강법을 최적화 함수로 사용하고, loss 값을 최소가 되게 만든다.\n"," # 데이터는 플레이스홀더 X에는 x_data를 넣어주고\n"," # 플레이스 홀더 Y에는 y_data를 넣어준다.\n"," # 값을 2개 반환 받게 되는데 loss값만 loss_val 변수로 저장하자.\n","  for step in range(1001):\n","    _, loss_val = sess.run([train_op,loss], feed_dict = {X:x_data, Y:y_data})\n"," # 10번마다 횟수, loss값, W값, b값을 확인해보자.\n","    if step%10 ==0:\n","      print(step, loss_val, sess.run(W), sess.run(b))\n","\n","  print('Y = {Weight} * X + {Bias}'.format(Weight = sess.run(W), Bias = sess.run(b)))\n"," # score 함수에 x값으로 5를 넣어 예측되는 Y값을 보자. X라는 플레이스홀더를 이용하자.\n","  print('X:5, Y:', sess.run(score, feed_dict={X:5}))\n"," # score 함수에 x값으로 2.5를 넣어 예측되는 Y값을 보자. X라는 플레이스홀더를 이용하자.\n","  print('X:2.5, Y:', sess.run(score, feed_dict={X:2.5}))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 838.1791 [2.9198914] [1.5177826]\n","10 5.564961 [8.719186] [3.0082488]\n","20 1.6877296 [9.135743] [3.011641]\n","30 1.5613322 [9.1896515] [2.9183145]\n","40 1.4590082 [9.218325] [2.8216033]\n","50 1.3634558 [9.244468] [2.7276764]\n","60 1.2741619 [9.269634] [2.6368477]\n","70 1.190717 [9.293957] [2.5490415]\n","80 1.1127368 [9.317467] [2.4641593]\n","90 1.0398638 [9.340195] [2.3821034]\n","100 0.97176 [9.362167] [2.3027797]\n","110 0.908119 [9.383407] [2.2260976]\n","120 0.8486476 [9.403939] [2.1519687]\n","130 0.79306835 [9.423789] [2.0803082]\n","140 0.7411286 [9.442976] [2.0110335]\n","150 0.6925913 [9.461525] [1.9440665]\n","160 0.6472324 [9.479456] [1.8793293]\n","170 0.6048454 [9.496789] [1.8167481]\n","180 0.56523347 [9.513546] [1.7562512]\n","190 0.52821606 [9.529745] [1.6977683]\n","200 0.49362335 [9.545405] [1.641233]\n","210 0.46129566 [9.560543] [1.5865798]\n","220 0.431084 [9.575176] [1.533747]\n","230 0.40285206 [9.589324] [1.4826736]\n","240 0.37646866 [9.602999] [1.4333007]\n","250 0.35181293 [9.616219] [1.3855718]\n","260 0.32877392 [9.628999] [1.3394326]\n","270 0.30724186 [9.641353] [1.2948298]\n","280 0.28712046 [9.6532955] [1.2517124]\n","290 0.26831675 [9.66484] [1.2100306]\n","300 0.2507441 [9.676002] [1.1697372]\n","310 0.23432246 [9.686791] [1.1307846]\n","320 0.21897718 [9.697221] [1.0931294]\n","330 0.20463529 [9.707303] [1.0567282]\n","340 0.1912336 [9.71705] [1.0215393]\n","350 0.17871031 [9.726472] [0.98752254]\n","360 0.16700569 [9.73558] [0.9546384]\n","370 0.15606864 [9.744385] [0.9228492]\n","380 0.14584777 [9.752896] [0.892119]\n","390 0.13629645 [9.7611265] [0.8624119]\n","400 0.1273704 [9.76908] [0.8336935]\n","410 0.119028494 [9.77677] [0.8059316]\n","420 0.11123352 [9.784204] [0.77909446]\n","430 0.10394875 [9.791389] [0.7531508]\n","440 0.09714082 [9.798336] [0.72807086]\n","450 0.090778865 [9.805052] [0.703826]\n","460 0.08483373 [9.8115425] [0.6803888]\n","470 0.07927847 [9.817819] [0.65773225]\n","480 0.074086376 [9.823886] [0.63583004]\n","490 0.069234155 [9.82975] [0.6146569]\n","500 0.06470005 [9.83542] [0.59418905]\n","510 0.060462486 [9.8409] [0.5744024]\n","520 0.05650289 [9.846198] [0.5552749]\n","530 0.05280254 [9.851319] [0.5367843]\n","540 0.049344745 [9.856271] [0.51890945]\n","550 0.04611305 [9.861056] [0.5016299]\n","560 0.043092694 [9.865684] [0.4849256]\n","570 0.04027059 [9.870156] [0.46877792]\n","580 0.03763334 [9.87448] [0.45316762]\n","590 0.035168607 [9.87866] [0.4380771]\n","600 0.03286535 [9.8827] [0.42348897]\n","610 0.030713152 [9.886606] [0.40938705]\n","620 0.028701406 [9.890383] [0.3957544]\n","630 0.026821833 [9.8940325] [0.38257563]\n","640 0.025065402 [9.897561] [0.369836]\n","650 0.023423862 [9.900972] [0.35752052]\n","660 0.021889836 [9.90427] [0.3456151]\n","670 0.020456204 [9.907457] [0.33410636]\n","680 0.019116322 [9.910539] [0.3229808]\n","690 0.01786465 [9.913519] [0.31222603]\n","700 0.01669459 [9.916398] [0.30182892]\n","710 0.015601464 [9.919182] [0.29177827]\n","720 0.014579485 [9.921874] [0.2820619]\n","730 0.013624743 [9.924476] [0.27266917]\n","740 0.0127322795 [9.9269905] [0.2635889]\n","750 0.011898394 [9.929422] [0.25481114]\n","760 0.011119205 [9.931772] [0.24632578]\n","770 0.01039115 [9.934044] [0.23812297]\n","780 0.009710382 [9.93624] [0.23019351]\n","790 0.00907455 [9.938363] [0.22252801]\n","800 0.008480113 [9.940416] [0.21511766]\n","810 0.007924731 [9.942401] [0.20795394]\n","820 0.007405834 [9.944318] [0.20102905]\n","830 0.0069206604 [9.946172] [0.19433503]\n","840 0.006467654 [9.947965] [0.18786383]\n","850 0.0060439776 [9.9496975] [0.18160799]\n","860 0.0056481906 [9.951373] [0.17556046]\n","870 0.0052783424 [9.9529915] [0.16971426]\n","880 0.0049325963 [9.954557] [0.16406281]\n","890 0.004609563 [9.95607] [0.15859964]\n","900 0.00430764 [9.957533] [0.15331833]\n","910 0.0040255673 [9.958948] [0.14821276]\n","920 0.00376182 [9.960315] [0.14327723]\n","930 0.0035154913 [9.961636] [0.13850592]\n","940 0.003285251 [9.9629135] [0.13389371]\n","950 0.0030700858 [9.9641485] [0.12943484]\n","960 0.00286907 [9.9653425] [0.12512462]\n","970 0.0026811343 [9.966496] [0.12095802]\n","980 0.0025055266 [9.967612] [0.1169301]\n","990 0.002341491 [9.968691] [0.1130363]\n","1000 0.0021881147 [9.969733] [0.10927219]\n","Y = [9.969733] * X + [0.10927219]\n","X:5, Y: [49.95794]\n","X:2.5, Y: [25.033606]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"80mxEC5oT4hK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}